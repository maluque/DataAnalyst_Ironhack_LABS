{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79efd7b2",
   "metadata": {},
   "source": [
    "Data cleaning functions TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc095e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69da16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_abs(df, ascend=False):\n",
    "    df1 = df.isna().sum()\n",
    "    df1=df1[df1!=0].sort_values(ascending = ascend)\n",
    "    return df1\n",
    "\n",
    "def na_perc(df, ascend=False):\n",
    "    df1 = df.isna().mean()*100\n",
    "    df1 = df1[df1!=0].sort_values(ascending = ascend)\n",
    "    return df1\n",
    "\n",
    "def na_absperc(df):\n",
    "    return pd.concat([na_cols_abs(df), na_cols_perc(df)], axis=1, keys= [\"abs_NA\", \"perc_NA\"])\n",
    "\n",
    "\n",
    "def categ_summ(df):\n",
    "    '''\n",
    "    Creates a modified version of 'describe objects' function\n",
    "    \n",
    "    Adds 3 new columns to evaluate the ratio between unique/level values\n",
    "    and their frequency\n",
    "    \n",
    "    \"resto_per\" column may pinpoint potential misspelled errors as\n",
    "    Top rows: indicate there is ONE LEVEL with EXCESIVE FREQ\n",
    "    Bottom rows: indicate there are MANY LEVELS with very LOW FREQ\n",
    "    \n",
    "    '''\n",
    "\n",
    "    sumdf=df.describe(include = \"object\").T\n",
    "    sumdf[\"unicount_ratio\"]=sumdf[\"unique\"]/sumdf[\"count\"]\n",
    "\n",
    "    sumdf[\"resto_abs\"]=(sumdf[\"count\"]-sumdf[\"freq\"])\n",
    "    sumdf[\"resto_per\"]=(sumdf[\"resto_abs\"]*100)/sumdf[\"count\"]\n",
    "\n",
    "    sumdf.sort_values([\"resto_per\", \"unique\"])\n",
    "    return sumdf\n",
    "\n",
    "\n",
    "\n",
    "def variance_check(df, perc_a, perc_b):\n",
    "    '''\n",
    "    Creates a modified version of 'describe numeric' function\n",
    "    \n",
    "    Adds 2 new columns to dispay quantile A and B defined by the user\n",
    "   \n",
    "    NOTE: The function will only filter and evaluate the NUMERIC COLUMNS!\n",
    "    perc_a and perc_b must be from 0-1\n",
    "    '''\n",
    "    subdf=df.select_dtypes(include='number')\n",
    "    sumdf=subdf.describe(include=\"number\").T\n",
    "\n",
    "    sumdf[\"P\" + str(int(perc_a*100))]=numeric_df.quantile(perc_a)\n",
    "    sumdf[\"P\" + str(int(perc_b*100))]=numeric_df.quantile(perc_b)\n",
    "\n",
    "    return sumdf.sort_values(\"std\", ascending = False)\n",
    "\n",
    "\n",
    "def outliers(df):\n",
    "    outliers = pd.DataFrame(columns=df.columns)\n",
    "    stats=df.describe().transpose()\n",
    "    stats['IQR'] = stats['75%'] - stats['25%']\n",
    "    \n",
    "    for col in stats.index:\n",
    "        iqr = stats.at[col,'IQR']\n",
    "        cutoff = iqr * 1.5\n",
    "        lower = stats.at[col,'25%'] - cutoff\n",
    "        upper = stats.at[col,'75%'] + cutoff\n",
    "        results = df[(df[col] < lower) | \n",
    "                       (df[col] > upper)].copy()\n",
    "        results['Outlier'] = col\n",
    "        outliers = outliers.append(results)\n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "def reduc_mem(df):\n",
    "    '''\n",
    "    reduce the memory usage of the dataframe by:\n",
    "    1),2) downcasting the int and float columns into numeric with lowest bits possible\n",
    "    3) collapsing object columns into cateory (factor levels)\n",
    "    \n",
    "    '''\n",
    "    dytpes_list=df.dtypes\n",
    "\n",
    "    for i in range(len(dytpes_list)):\n",
    "        if dytpes_list[i]==\"int\" :\n",
    "            df[df.columns[i]] = pd.to_numeric(df[df.columns[i]], downcast='integer')\n",
    "        elif dytpes_list[i]==\"float\" :\n",
    "            df[df.columns[i]] = pd.to_numeric(df[df.columns[i]], downcast='float')\n",
    "        elif dytpes_list[i]==\"object\" :\n",
    "            df[df.columns[i]] = df[df.columns[i]].astype('category')\n",
    "        else:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def check_nan(df: pd.DataFrame) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Recibe un dataframe y enseña el % de valores nulos\n",
    "    y lo grafica\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_cols = df.isna().mean() * 100  # porcentaje de nulo en cada columna\n",
    "    \n",
    "    display(f'N nan cols: {len(nan_cols[nan_cols>0])}')\n",
    "    display(nan_cols[nan_cols>0])\n",
    "    \n",
    "    \n",
    "    # grafico de nulos en el dataframe\n",
    "    #inicializa figura y establece un tamaño\n",
    "    plt.figure(figsize=(10, 6)) # 100x60 pixeles\n",
    "\n",
    "    sns.heatmap(df.isna(),          # datos\n",
    "                yticklabels=False,  # quita las etiquetas del eje y\n",
    "                cmap='viridis',     # mapa de color\n",
    "                cbar=False,         # sin barra lateral\n",
    "               )\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9aeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a8572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
